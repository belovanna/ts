{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Time series models on example of TESLA stock prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies:\n",
    "    - pandas\n",
    "    - pandas-datareader\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - scikit-learn\n",
    "    - seaborn\n",
    "    - datetime\n",
    "    - warnings\n",
    "    - sys\n",
    "    - itertools\n",
    "    - statsmodels\n",
    "    - pyramid-arima (pip install pyramid-arima)\n",
    "    - keras\n",
    "    - pykalman \n",
    "    - bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/tesla.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the excercise is to create a time-series model that tracks close enough to the closing price of Tesla stocks each day. Tesla is a fascinating case for the analysis because it has been the most valuable car manufacturing company in the USA in 2017 despite selling only 4 car models. Besides, Tesla has never had a profitable year, so it's stock price is speculative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA model\n",
    "First, we can see if ARIMA models are capable of capturing such turbulent series. We'll follow the standard path:\n",
    "+ Visualize the time series\n",
    "+ Stationarize the series\n",
    "+ Plot ACF/PACF charts and find optimal parameters\n",
    "+ Build the ARIMA model\n",
    "+ Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "import pandas_datareader.data as web\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import sys\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from pyramid.arima import auto_arima\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tsla dataset contains daily data on TSLA stocks from 2013-8-15 to 2018-8-15. We're interested in Close price series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data\n",
    "dateparse = lambda date: pd.datetime.strptime(date, '%Y-%m-%d')\n",
    "tsla = pd.read_csv('assets/data/tsla', parse_dates=['Date'], index_col='Date', date_parser=dateparse,)\n",
    "tsla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the daily Close price\n",
    "tsla.Close.plot(lw=2.5, figsize=(12,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as we can tell from this plot, the series for Close stock price look non stationary, with no obvious linear trend or seasonality. The distribution is bimodal (with two peaks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution plot of tsla series\n",
    "tsla_close = pd.Series.to_frame(tsla.Close)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(tsla_close.dropna(), color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to decompose the data into trend and seasonal components. By changing frequency, we can check for the presence weekly, monthly or yearly seasonal fluctuations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decompose into seasonal and trend components\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(tsla_close, freq=30, model='additive')  \n",
    "fig = plt.figure()  \n",
    "fig = decomposition.plot()  \n",
    "fig.set_size_inches(15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing out data with moving average\n",
    "Rather than calculating the average on the whole dataset, moving average (also called rolling mean) calculates the average of a subset with a certain window size, and shifts forward. Moving average is used to smooth out short-term fluctuations and highlight longer-term trends or cycles.\n",
    "Let's see how moving average works by taking a shorter recent time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_recent = tsla_close.loc['2018-5-15':'2018-8-15']\n",
    "rroll_d3 = tsla_recent.rolling(window=3).mean()\n",
    "rroll_d7 = tsla_recent.rolling(window=7).mean()\n",
    "rroll_d14 = tsla_recent.rolling(window=14).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(tsla_recent.index, tsla_recent, lw=3, alpha=0.8,label='Original observations')\n",
    "plt.plot(tsla_recent.index, rroll_d3, lw=3, alpha=0.8,label='Rolling mean (window 3)')\n",
    "plt.plot(tsla_recent.index, rroll_d7, lw=3, alpha=0.8,label='Rolling mean (window 7)')\n",
    "plt.plot(tsla_recent.index, rroll_d14, lw=3, alpha=0.8,label='Rolling mean (window 14)')\n",
    "plt.title('Tesla Close Price 2013-5-15 to 2018-8-15')\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.legend(loc='upper left', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test for stationarity and stationarize the data\n",
    "The Augmented Dicky Fuller test is a type of statistical test called a unit root test.\n",
    "The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend.\n",
    "There are no. of unit root tests and ADF is one of the most widely used\n",
    "1. Null Hypothesis (H0): Null hypothesis of the test is that the time series can be represented by a unit root that is not stationary.\n",
    "2. Alternative Hypothesis (H1): Alternative Hypothesis of the test is that the time series is stationary.\n",
    "Interpretation of p value\n",
    "1. p value > 0.05: Accepts the Null Hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "2. p value < = 0.05: Rejects the Null Hypothesis (H0), the data is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adf test for stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(x):\n",
    "#Determing rolling statistics\n",
    "    rolmean = x.rolling(window=22,center=False).mean()\n",
    "    rolstd = x.rolling(window=12,center=False).std()\n",
    "    \n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(x, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "\n",
    "#Perform Dickey Fuller test    \n",
    "    result=adfuller(x)\n",
    "    print('ADF Stastistic: %f'%result[0])\n",
    "    print('p-value: %f'%result[1])\n",
    "    pvalue=result[1]\n",
    "    for key,value in result[4].items():\n",
    "         if result[0]>value:\n",
    "            print(\"The graph is non stationary\")\n",
    "            break\n",
    "         else:\n",
    "            print(\"The graph is stationary\")\n",
    "            break;\n",
    "    print('Critical values:')\n",
    "    for key,value in result[4].items():\n",
    "        print('\\t%s: %.3f ' % (key, value))\n",
    "ts = tsla.Close\n",
    "test_stationarity(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p value is greater than 0.05 the time series is non stationary.\n",
    "Let's try transforming the data.\n",
    "#### Log transformation of the series\n",
    "is typically used to unskew highly skewed data, thus helping in forecasting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform series and test stationarity\n",
    "ts_log = np.log(ts)\n",
    "plt.plot(ts_log, color = \"green\")\n",
    "plt.show()\n",
    "test_stationarity(ts_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series remain non stationary as p-value is still greater than 0.05 and some further transformations can be applied. Let's try to go ahead and use differencing. The log differencing of the series has the meaning. We transform price levels into the rate of returns. The reason for multiplying by 100 is due to numerical problems in the estimation\n",
    "part. This will not affect the structure of the model since it is just a linear scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take first differences and test stationarity\n",
    "ts_log_diff = (ts_log - ts_log.shift())*100\n",
    "plt.plot(ts_log_diff, color = \"green\")\n",
    "plt.show()\n",
    "ts_log_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for stationarity\n",
    "ts_log_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(ts_log_diff.dropna(), color='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series is now stationary as p value is less than 0.05. The distributions of stock returns (log difference of stock prices) is close to normal, with mean zero. We can now proceed with fitting ARIMA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot and inspect the empirical autocorrelation function\n",
    "As we remember from the lecture, the ACF of a stationary AR process of order p goes to zero at an exponential rate, while the PACF becomes zero after lag p. For an MA process of order q the theoretical ACF and PACF show the reverse behaviour, the ACF truncates after lag q and the PACF goes to zero at an exponential rate. These properties can be used as a guide to choose the orders of an ARMA model. Here in this data we observe the presence of both AR and MA behaviours. Thus, ARIMA might give the most parsimonius representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots of acf/pacf\n",
    "plt.figure(1, figsize=(12,15))\n",
    "plt.subplot(211)\n",
    "plot_acf(ts_log_diff,ax=plt.gca(),lags = range(1, 200))\n",
    "plt.subplot(212)\n",
    "plot_pacf(ts_log_diff, ax=plt.gca(), lags = range(1, 200))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the triplets of parameters defined above to automate the process of training and evaluating ARIMA models on different combinations. In Statistics and Machine Learning, this process is known as grid search (or hyperparameter optimization) for model selection.\n",
    "\n",
    "When evaluating and comparing statistical models fitted with different parameters, each can be ranked against one another based on how well it fits the data or its ability to accurately predict future data points. We will use the AIC (Akaike Information Criterion) and BIC (Bayesian information criterion) values, which is conveniently returned with ARIMA models fitted using statsmodels. The AIC and BIC measure how well a model fits the data while taking into account the overall complexity of the model. A model that fits the data very well while using lots of features will be assigned a larger AIC/BIC score than a model that uses fewer features to achieve the same goodness-of-fit. Therefore, we are interested in finding the model that yields the lowest AIC/BIC values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit autoarima\n",
    "stepwise_fit = auto_arima(ts_log, start_p=0, start_q=0, max_p=20, max_q=10, m=7,\n",
    "                      start_P=0, seasonal=False, trace=True,\n",
    "                      error_action='ignore',  # don't want to know if an order does not work\n",
    "                      suppress_warnings=False,  # don't want convergence warnings\n",
    "                      stepwise=True, maxiter=200)  # set to stepwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum AIC is achieved for ARIMA(1,1,1). Let's fit this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit arima(1,1,1)\n",
    "tsla_close.index = pd.DatetimeIndex(ts_log.index.values,\n",
    "                               freq=tsla_close.index.inferred_freq)\n",
    "model = ARIMA(np.log(tsla_close), order=(1,1,1))  \n",
    "results = model.fit(disp=0)\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual errors\n",
    "residuals = DataFrame(results.resid)\n",
    "residuals.plot(figsize=(12,5))\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(residuals.dropna(), color='g')\n",
    "plt.show()\n",
    "\n",
    "# qq plot\n",
    "import scipy.stats as stats\n",
    "stats.probplot(results.resid, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q plot\")\n",
    "plt.show()\n",
    "\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are drawn from a distribution that is over-dispersed relative to a normal distribution. Over-dispersed data has an increased number of outliers (i.e. the distribution has fatter tails than a normal distribution). Over-dispersed data is also known as having a leptokurtic distribution and as having positive excess kurtosis. On a Q-Q plot over-dispersed data appears as a flipped S shape (the opposite of under-dispersed data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Forecast with ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train and test\n",
    "size = int(len(ts_log)-14)\n",
    "train_arima, test_arima = ts_log[0:size], ts_log[size:len(ts_log)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions based on ARIMA(1,1,1)\n",
    "history = [x for x in train_arima]\n",
    "predictions = list()\n",
    "originals = list()\n",
    "error_list = list()\n",
    "\n",
    "print('Printing Predicted vs Expected Values...')\n",
    "print('\\n')\n",
    "for t in range(len(test_arima)):\n",
    "    model = ARIMA(history, order=(1, 1, 1))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    \n",
    "    output = model_fit.forecast()\n",
    "    \n",
    "    pred_value = output[0]\n",
    "    \n",
    "        \n",
    "    original_value = test_arima[t]\n",
    "    history.append(original_value)\n",
    "    \n",
    "    pred_value = np.exp(pred_value)\n",
    "    \n",
    "    \n",
    "    original_value = np.exp(original_value)\n",
    "    \n",
    "    # Calculating the error\n",
    "    error = ((abs(pred_value - original_value)) / original_value) * 100\n",
    "    error_list.append(error)\n",
    "    print('predicted = %f,   expected = %f,   error = %f ' % (pred_value, original_value, error), '%')\n",
    "    \n",
    "    predictions.append(float(pred_value))\n",
    "    originals.append(float(original_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After iterating over entire test set the overall mean error is calculated.   \n",
    "print('\\n Mean Error in Predicting Test Case Articles : %f ' % (sum(error_list)/float(len(error_list))), '%')\n",
    "\n",
    "#Plot 1-day ahead forecast vs the real value\n",
    "plt.figure(figsize=(8, 6))\n",
    "test_day = [t\n",
    "           for t in range(len(test_arima))]\n",
    "labels={'Orginal','Predicted'}\n",
    "plt.plot(test_day, predictions, color= 'green')\n",
    "plt.plot(test_day, originals, color = 'orange')\n",
    "plt.title('Expected Vs Predicted Views Forecasting')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.legend(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/musk.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The оne period ahead forecast seems to be close to the previous period value. It's because if you inspect the AR(1) coefficient, you can notice that it's close to 0.9871, so essentially the series are very close to the random walk process (the next value equals to the previous value plus some random error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network to Predict Stock Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we run a RNN on the same series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file into training set\n",
    "training_set = pd.read_csv('assets/data/tsla_train')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file into test set\n",
    "test_set = pd.read_csv('assets/data/tsla_test')\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting relevant feature\n",
    "training_set = training_set.iloc[:,2:3]\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to 2D array\n",
    "training_set = training_set.values\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "training_set = sc.fit_transform(training_set)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the inputs and the ouputs\n",
    "X_train = training_set[0:1043]\n",
    "y_train = training_set[1:1044]\n",
    "\n",
    "# Example\n",
    "today = pd.DataFrame(X_train[0:5])\n",
    "tomorrow = pd.DataFrame(y_train[0:5])\n",
    "ex = pd.concat([today, tomorrow], axis=1)\n",
    "ex.columns = (['today', 'tomorrow'])\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping into required shape for Keras\n",
    "X_train = np.reshape(X_train, (1043, 1, 1))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Recurrent Neural Network\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 4, activation = 'sigmoid', input_shape = (None, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the Recurrent Neural Network\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Recurrent Neural Network to the Training set\n",
    "history=regressor.fit(X_train, y_train, batch_size = 32, epochs = 200, validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting relevant feature# Gettin \n",
    "real_stock_price = test_set.iloc[:,2:3]\n",
    "real_stock_price.head()\n",
    "#len(real_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to 2D array\n",
    "real_stock_price = real_stock_price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "inputs = real_stock_price\n",
    "inputs = sc.transform(inputs)\n",
    "inputs = np.reshape(inputs, (261, 1, 1))\n",
    "predicted_stock_price = regressor.predict(inputs)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from datetime import date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "output_notebook()\n",
    "df = test_set\n",
    "df.head()\n",
    "df['date'] = pd.to_datetime(df.Date)\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "\n",
    "p = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=1000, toolbar_location=\"left\",y_axis_label = \"TSLA Price\",\n",
    "          x_axis_label = \"Date\")\n",
    "\n",
    "p.line(df.date,predicted_stock_price.flatten(),line_width=1,line_color = 'blue',legend=\"Real TSLA Stock Price\")\n",
    "p.line(df.date,real_stock_price.flatten(),line_width=1,line_color = 'red',legend=\"TSLA price test dataset\")\n",
    "\n",
    "p.title.text = 'LSTM forecast vs actual stock prices'\n",
    "p.xaxis.major_label_orientation = pi/4\n",
    "p.grid.grid_line_alpha=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman filter excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pykalman import KalmanFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = real_stock_price\n",
    "y_pred = predicted_stock_price\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KalmanFilter(transition_matrices = [1],\n",
    "                  observation_matrices = [1],\n",
    "                  initial_state_mean = df['Close'].values[0],\n",
    "                  initial_state_covariance = 1,\n",
    "                  observation_covariance=1,\n",
    "                  transition_covariance=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_means,_ = kf.filter(df[['Close']].values)\n",
    "state_means = state_means.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.Date)\n",
    "\n",
    "mids = (df.Open + df.Close)/2\n",
    "spans = abs(df.Close-df.Open)\n",
    "\n",
    "inc = df.Close > df.Open\n",
    "dec = df.Open > df.Close\n",
    "w = 12*60*60*1000 # half day in ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "\n",
    "p = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=1000, toolbar_location=\"left\",y_axis_label = \"Price\",\n",
    "          x_axis_label = \"Date\")\n",
    "\n",
    "p.segment(df.date, df.High, df.date, df.Low, color=\"black\")\n",
    "p.rect(df.date[inc], mids[inc], w, spans[inc], fill_color='green', line_color=\"green\")\n",
    "p.rect(df.date[dec], mids[dec], w, spans[dec], fill_color='red', line_color=\"red\")\n",
    "p.line(df.date,state_means,line_width=1,line_color = 'blue',legend=\"Kalman filter\")\n",
    "p.line(df.date,real_stock_price.flatten(),line_width=1.4,line_color = 'black',legend=\"TSLA price test dataset\")\n",
    "\n",
    "p.title.text = 'Implementation of Kalman Filter Smoothing'\n",
    "p.xaxis.major_label_orientation = pi/4\n",
    "p.grid.grid_line_alpha=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = real_stock_price.flatten()\n",
    "y_pred = state_means\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
